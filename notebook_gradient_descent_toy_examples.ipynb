{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for simple one and 2 variable examples and visualizations of gradient descent, momentum and linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"visualizations\"):\n",
    "    os.mkdir(\"visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a simple gradient descent algorithm for the function\n",
    "$$y=x^2$$\n",
    "\n",
    "The actual minimum is $x=0$\n",
    "\n",
    "Sources:\n",
    "- https://nbviewer.jupyter.org/url/courses.d2l.ai/berkeley-stat-157/slides/4_30/gd-sgd.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(lr, initial_x, epochs):\n",
    "    x = initial_x\n",
    "    x_values = [x]\n",
    "    for i in range(epochs):\n",
    "        x = x - lr * 2 * x # derivative of x^2\n",
    "        x_values.append(x)\n",
    "    return x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(ax, x_values, func):\n",
    "    n = max(abs(min(x_values)), abs(max(x_values)))\n",
    "    f = np.arange(-n, n, 0.1)\n",
    "    ax.plot(f, [func(x) for x in f]) # the actual function\n",
    "    ax.plot(x_values, [func(x) for x in x_values], \"-o\", color=\"red\") # the values calculated for x after every gd epoch\n",
    "    ax.grid()\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x = -10\n",
    "epochs = 10\n",
    "func = lambda x: x**2\n",
    "learning_rates = [0.01, 0.2, 0.8, 0.99]\n",
    "infos = [\"too small\", \"good\", \"too large\", \"way too large\"]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7), gridspec_kw=dict(hspace=0.35))\n",
    "fig.suptitle(r\"Gradient Descent behaviour for $y=x^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axs[i, k]\n",
    "    ax.set_title(f\"lr={lr} ({infos[idx]})\")\n",
    "    x_values = gd(lr, initial_x=initial_x, epochs=epochs)\n",
    "    show_trace(ax=ax, x_values=x_values, func=func)\n",
    "    k = (k + 1) % 2\n",
    "    if k == 0:\n",
    "        i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/gradient_descent_1d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the difference between gradient descent and stochastic gradient descent for the multivariate function\n",
    "$$y=x_1^2+2x_2^2$$\n",
    "\n",
    "The actual minimum is $x_1=0$ and $x_2=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_2d(lr, initial_point, epochs):\n",
    "    p = initial_point\n",
    "    results = [p]\n",
    "    for i in range(epochs):\n",
    "        x1 = p[0] - lr * 2 * p[0] # partial derivative for x1\n",
    "        x2 = p[1] - lr * 4 * p[1] # partial derivate for x2\n",
    "        p = (x1, x2)\n",
    "        results.append(p)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace_2d(ax, points, func, x1lim=(-5.5, 1.0), x2lim=(-2.5, 0.5)):\n",
    "    x1, x2 = np.arange(x1lim[0], x1lim[1], 0.1), np.arange(x2lim[0], x2lim[1], 0.1)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Z = func(X1, X2)\n",
    "    labels = ax.contour(X1, X2, Z, 40)\n",
    "    ax.autoscale(False)\n",
    "    for p in points:\n",
    "        ax.plot([p[0] for p in points], [p[1] for p in points], \"-o\", color=\"red\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    # ax.clabel(labels, inline=1, fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = (-5, -2)\n",
    "epochs = 20\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "infos = [\"too small\", \"good\", \"too large\", \"way too large\"]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7), gridspec_kw=dict(hspace=0.35))\n",
    "fig.suptitle(r\"Gradient Descent behaviour for $y=x_1^2+2x_2^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "func = lambda X1, X2: X1**2 + 2*X2**2\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axs[i, k]\n",
    "    ax.set_title(f\"lr={lr} ({infos[idx]})\")\n",
    "    points = gd_2d(lr, initial_point=initial_point, epochs=epochs)\n",
    "    show_trace_2d(axs[i, k], points, func)\n",
    "    k = (k + 1) % 2\n",
    "    if k == 0:\n",
    "        i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/gradient_descent_2d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing for momentum\n",
    "\n",
    "Sources:\n",
    "- https://nbviewer.jupyter.org/url/courses.d2l.ai/berkeley-stat-157/slides/5_2/momentum.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_2d(lr, gamma, initial_point, epochs):\n",
    "    prev_x1, prev_x2 = 0, 0\n",
    "    p = initial_point\n",
    "    results = [p]\n",
    "    for i in range(epochs):\n",
    "        prev_x1 = gamma * prev_x1 + lr * 2 * p[0] # partial derivative for x1\n",
    "        prev_x2 = gamma * prev_x2 + lr * 4 * p[1] # partial derivative for x2\n",
    "        x1 = p[0] - prev_x1\n",
    "        x2 = p[1] - prev_x2\n",
    "        p = (x1, x2)\n",
    "        results.append(p)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = (-5, -2)\n",
    "epochs = 20\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "gammas = [0.4, 0.5, 0.9, 0.99]\n",
    "fig, axs = plt.subplots(4, 4, figsize=(25, 14), gridspec_kw=dict(hspace=0.4))\n",
    "fig.suptitle(\"Gradient Descent with momentum\\n\"r\"behaviour for $y=x_1^2+2x_2^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "func = lambda X1, X2: X1**2 + 2*X2**2\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    for kdx, gamma in enumerate(gammas):\n",
    "        ax = axs[i, k]\n",
    "        ax.set_title(f\"lr={lr}\" r\"$\\gamma$\"f\"={gamma}\")\n",
    "        points = momentum_2d(lr, gamma=gamma, initial_point=initial_point, epochs=epochs)\n",
    "        show_trace_2d(axs[i, k], points, func, x1lim=(-5.5, 3.0), x2lim=(-2.5, 3.0))\n",
    "        k = (k + 1) % 4\n",
    "        if k == 0:\n",
    "            i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/momentum_2d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Descent for Linear Regression\n",
    "\n",
    "Sources:\n",
    "- https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "np.random.seed(666)\n",
    "truth = lambda x: 1.5*x - 20\n",
    "amount = 250\n",
    "X = range(0, amount)\n",
    "target_line = [truth(val) for val in X]\n",
    "training_data = np.random.normal(loc=target_line, scale=amount/3, size=(1, amount))\n",
    "plt.scatter(X, training_data)\n",
    "plt.plot(target_line, color=\"lime\", linewidth=4)\n",
    "plt.title(r\"training data around truth function $y=\\dfrac{3}{2}x-20$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model y = m * x + c\n",
    "regression_model = lambda m, c, x: m * x + c\n",
    "\n",
    "# loss function: mean squared error (mse)\n",
    "def mse(training_data, predictions):\n",
    "    squared_error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[0, idx] - prediction # precalculated prediction = m * idx + c\n",
    "        squared_error_sum +=  diff * diff\n",
    "    mean = squared_error_sum / len(predictions)\n",
    "    return mean\n",
    "\n",
    "def partial_derivative_mse_m(training_data, predictions):\n",
    "    error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[0, idx] - prediction\n",
    "        error_sum += idx * diff\n",
    "    return -2 / len(predictions) * error_sum\n",
    "\n",
    "def partial_derivative_mse_c(training_data, predictions):\n",
    "    error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[0, idx] - prediction\n",
    "        error_sum += diff\n",
    "    return -2 / len(predictions) * error_sum\n",
    "\n",
    "def calc_predictions_and_loss(m, c, X, training_data):\n",
    "    predictions = [regression_model(m, c, x) for x in X]\n",
    "    return predictions, mse(training_data, predictions)\n",
    "\n",
    "# training\n",
    "def train_regression_model(epochs, lr=0.0001, initial_m=0, initial_c=0):\n",
    "    m, c = initial_m, initial_c\n",
    "    loss_per_epoch = []\n",
    "    for i in range(epochs):\n",
    "        predictions, loss = calc_predictions_and_loss(m, c, X, training_data)\n",
    "        loss_per_epoch.append(loss)\n",
    "        m = m - lr * partial_derivative_mse_m(training_data, predictions)\n",
    "        c = c - lr * partial_derivative_mse_c(training_data, predictions)\n",
    "    return m, c, loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5), gridspec_kw=dict(hspace=0.35))\n",
    "left = axs[0]\n",
    "right = axs[1]\n",
    "fig.suptitle(f\"Result of training the regression model for {epochs} epochs\", y=1.15)\n",
    "\n",
    "m, c, loss_per_epoch = train_regression_model(epochs=epochs, lr=0.00001)\n",
    "\n",
    "predictions, _ = calc_predictions_and_loss(m, c, X, training_data)\n",
    "left.scatter(X, training_data)\n",
    "left.plot(target_line, color=\"lime\", linewidth=4)\n",
    "left.plot(predictions, color=\"red\", linewidth=4)\n",
    "left.set_title(r\"training data around truth function $y=\\dfrac{3}{2}x-20$\" + \"\\n\"\n",
    "               + r\"and learned function $y=mx+c$\" + \"\\nwith m = \"\n",
    "               + f\"{round(m, 4)} c = {round(c, 4)}\")\n",
    "left.legend([\"truth line\", \"predicted line\", \"training data\"])\n",
    "left.set_xlabel(\"x\")\n",
    "left.set_ylabel(\"y\")\n",
    "\n",
    "right.plot(range(0, len(loss_per_epoch)), loss_per_epoch)\n",
    "right.set_title(f\"loss from Mean Squared Error\\nwith latest loss: {round(loss_per_epoch[-1], 4)}\")\n",
    "right.set_xlabel(\"epoch\")\n",
    "right.set_ylabel(\"loss\")\n",
    "\n",
    "plt.savefig(\"./visualizations/gradient_descent_linear_regression_example.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
