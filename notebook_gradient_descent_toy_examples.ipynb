{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for simple one and 2 variable examples and visualizations of gradient descent, momentum and linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"visualizations\"):\n",
    "    os.mkdir(\"visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a simple gradient descent algorithm for the function\n",
    "$$y=x^2$$\n",
    "\n",
    "The actual minimum is $x=0$\n",
    "\n",
    "Sources:\n",
    "- https://nbviewer.jupyter.org/url/courses.d2l.ai/berkeley-stat-157/slides/4_30/gd-sgd.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(lr, initial_x, epochs):\n",
    "    x = initial_x\n",
    "    x_values = [x]\n",
    "    for i in range(epochs):\n",
    "        x = x - lr * 2 * x # derivative of x^2\n",
    "        x_values.append(x)\n",
    "    return x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(ax, x_values, func):\n",
    "    n = max(abs(min(x_values)), abs(max(x_values)))\n",
    "    f = np.arange(-n, n, 0.1)\n",
    "    ax.plot(f, [func(x) for x in f]) # the actual function\n",
    "    ax.plot(x_values, [func(x) for x in x_values], \"-o\", color=\"red\") # the values calculated for x after every gd epoch\n",
    "    ax.grid()\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_x = -10\n",
    "epochs = 10\n",
    "func = lambda x: x**2\n",
    "learning_rates = [0.01, 0.2, 0.8, 0.99]\n",
    "infos = [\"too small\", \"good\", \"too large\", \"way too large\"]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7), gridspec_kw=dict(hspace=0.35))\n",
    "fig.suptitle(r\"Gradient Descent behaviour for $y=x^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axs[i, k]\n",
    "    ax.set_title(f\"lr={lr} ({infos[idx]})\")\n",
    "    x_values = gd(lr, initial_x=initial_x, epochs=epochs)\n",
    "    show_trace(ax=ax, x_values=x_values, func=func)\n",
    "    k = (k + 1) % 2\n",
    "    if k == 0:\n",
    "        i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/gradient_descent_1d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the difference between gradient descent and stochastic gradient descent for the multivariate function\n",
    "$$y=x_1^2+2x_2^2$$\n",
    "\n",
    "The actual minimum is $x_1=0$ and $x_2=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_2d(lr, initial_point, epochs):\n",
    "    p = initial_point\n",
    "    results = [p]\n",
    "    for i in range(epochs):\n",
    "        x1 = p[0] - lr * 2 * p[0] # partial derivative for x1\n",
    "        x2 = p[1] - lr * 4 * p[1] # partial derivate for x2\n",
    "        p = (x1, x2)\n",
    "        results.append(p)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace_2d(ax, points, func, x1lim=(-5.5, 1.0), x2lim=(-2.5, 0.5)):\n",
    "    x1, x2 = np.arange(x1lim[0], x1lim[1], 0.1), np.arange(x2lim[0], x2lim[1], 0.1)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Z = func(X1, X2)\n",
    "    labels = ax.contour(X1, X2, Z, 40)\n",
    "    ax.autoscale(False)\n",
    "    for p in points:\n",
    "        ax.plot([p[0] for p in points], [p[1] for p in points], \"-o\", color=\"red\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    # ax.clabel(labels, inline=1, fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = (-5, -2)\n",
    "epochs = 20\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "infos = [\"too small\", \"good\", \"too large\", \"way too large\"]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7), gridspec_kw=dict(hspace=0.35))\n",
    "fig.suptitle(r\"Gradient Descent behaviour for $y=x_1^2+2x_2^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "func = lambda X1, X2: X1**2 + 2*X2**2\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axs[i, k]\n",
    "    ax.set_title(f\"lr={lr} ({infos[idx]})\")\n",
    "    points = gd_2d(lr, initial_point=initial_point, epochs=epochs)\n",
    "    show_trace_2d(axs[i, k], points, func)\n",
    "    k = (k + 1) % 2\n",
    "    if k == 0:\n",
    "        i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/gradient_descent_2d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same thing for momentum\n",
    "\n",
    "Sources:\n",
    "- https://nbviewer.jupyter.org/url/courses.d2l.ai/berkeley-stat-157/slides/5_2/momentum.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_2d(lr, gamma, initial_point, epochs):\n",
    "    prev_x1, prev_x2 = 0, 0\n",
    "    p = initial_point\n",
    "    results = [p]\n",
    "    for i in range(epochs):\n",
    "        prev_x1 = gamma * prev_x1 + lr * 2 * p[0] # partial derivative for x1\n",
    "        prev_x2 = gamma * prev_x2 + lr * 4 * p[1] # partial derivative for x2\n",
    "        x1 = p[0] - prev_x1\n",
    "        x2 = p[1] - prev_x2\n",
    "        p = (x1, x2)\n",
    "        results.append(p)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = (-5, -2)\n",
    "epochs = 20\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "gammas = [0.4, 0.5, 0.9, 0.99]\n",
    "fig, axs = plt.subplots(4, 4, figsize=(25, 14), gridspec_kw=dict(hspace=0.4))\n",
    "fig.suptitle(\"Gradient Descent with momentum\\n\"r\"behaviour for $y=x_1^2+2x_2^2$\"f\" and different learning rates ({epochs} epochs)\")\n",
    "func = lambda X1, X2: X1**2 + 2*X2**2\n",
    "i = 0\n",
    "k = 0\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    for kdx, gamma in enumerate(gammas):\n",
    "        ax = axs[i, k]\n",
    "        ax.set_title(f\"lr={lr}\" r\"$\\gamma$\"f\"={gamma}\")\n",
    "        points = momentum_2d(lr, gamma=gamma, initial_point=initial_point, epochs=epochs)\n",
    "        show_trace_2d(axs[i, k], points, func, x1lim=(-5.5, 3.0), x2lim=(-2.5, 3.0))\n",
    "        k = (k + 1) % 4\n",
    "        if k == 0:\n",
    "            i += 1\n",
    "\n",
    "plt.savefig(\"visualizations/momentum_2d_example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Descent for Linear Regression\n",
    "\n",
    "Sources:\n",
    "- https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "np.random.seed(666)\n",
    "truth = lambda x: 1.5*x - 20\n",
    "amount = 250\n",
    "X = range(0, amount)\n",
    "target_line = [truth(val) for val in X]\n",
    "training_data = np.random.normal(loc=target_line, scale=amount/3, size=(1, amount))\n",
    "training_data = training_data[0, :]\n",
    "plt.scatter(X, training_data)\n",
    "plt.plot(target_line, color=\"lime\", linewidth=4)\n",
    "plt.title(r\"training data around truth function $y=\\dfrac{3}{2}x-20$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model y = m * x + c\n",
    "regression_model = lambda m, c, x: m * x + c\n",
    "\n",
    "# data loader that enables either linear or shuffled loading\n",
    "def build_data_loader(training_data, params={\"type\": \"linear\"}):\n",
    "    current_idx = 0\n",
    "    total = len(training_data)\n",
    "    served_amount = 0\n",
    "    \n",
    "    def linear():\n",
    "        for idx, val in enumerate(training_data):\n",
    "            yield idx, val\n",
    "    \n",
    "    shuffled_copy = None\n",
    "    indices = None\n",
    "    def shuffled():\n",
    "        nonlocal shuffled_copy\n",
    "        if shuffled_copy == None:\n",
    "            shuffled_copy = list(enumerate(np.copy(training_data)))\n",
    "            np.random.shuffle(shuffled_copy)\n",
    "        for idx, val in shuffled_copy:\n",
    "            yield idx, val\n",
    "    \n",
    "    def mini_batch(batch_size):\n",
    "        nonlocal shuffled_copy, current_idx, total, served_amount\n",
    "        if shuffled_copy == None:\n",
    "            shuffled_copy = list(enumerate(np.copy(training_data)))\n",
    "            np.random.shuffle(shuffled_copy)\n",
    "        \n",
    "        while served_amount < total:\n",
    "            batch = shuffled_copy[current_idx:current_idx + batch_size]\n",
    "            current_idx += batch_size\n",
    "            served_amount += batch_size\n",
    "            yield batch\n",
    "        \n",
    "    \n",
    "    data_loader = None\n",
    "    if params[\"type\"] == \"linear\":\n",
    "        data_loader = {\"type\": \"linear\", \"loader\": linear}\n",
    "    elif params[\"type\"] == \"shuffled\":\n",
    "        data_loader = {\"type\": \"shuffled\", \"loader\": shuffled}\n",
    "    elif params[\"type\"] == \"mini-batch\" and \"batch-size\" in params:\n",
    "        data_loader = {\"type\": \"mini-batch\", \"loader\": lambda x=0: mini_batch(params[\"batch-size\"])}\n",
    "        \n",
    "    data_loader[\"get_resetted_data_loader\"] = lambda x=0: build_data_loader(training_data, params)\n",
    "    \n",
    "    return data_loader\n",
    "    \n",
    "# loss function: mean squared error (mse)\n",
    "def mse(training_data, predictions):\n",
    "    squared_error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[idx] - prediction # precalculated prediction = m * idx + c\n",
    "        squared_error_sum +=  diff * diff\n",
    "    mean = squared_error_sum / len(predictions)\n",
    "    return mean\n",
    "\n",
    "def partial_derivative_mse_m(training_data, predictions):\n",
    "    error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[idx] - prediction\n",
    "        error_sum += idx * diff\n",
    "    return -2 / len(predictions) * error_sum\n",
    "\n",
    "def partial_derivative_mse_c(training_data, predictions):\n",
    "    error_sum = 0\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        diff = training_data[idx] - prediction\n",
    "        error_sum += diff\n",
    "    return -2 / len(predictions) * error_sum\n",
    "\n",
    "def calc_predictions_and_loss(m, c, X, training_data):\n",
    "    predictions = [regression_model(m, c, x) for x in X]\n",
    "    return predictions, mse(training_data, predictions)\n",
    "\n",
    "def calc_prediction_and_loss(m, c, x, example):\n",
    "    prediction = regression_model(m, c, x)\n",
    "    return prediction, mse([example], [prediction])\n",
    "\n",
    "# training\n",
    "def train_regression_model(epochs, data_loader=None, amount=-1, lr=0.0001, initial_m=0, initial_c=0):\n",
    "    m, c = initial_m, initial_c\n",
    "    loss_per_epoch = []\n",
    "    for i in range(epochs):\n",
    "        if data_loader == None: # calculate loss and predictions over all examples and do only one update of m and c.\n",
    "            predictions, loss = calc_predictions_and_loss(m, c, X, training_data)\n",
    "            loss_per_epoch.append(loss)\n",
    "            m = m - lr * partial_derivative_mse_m(training_data, predictions)\n",
    "            c = c - lr * partial_derivative_mse_c(training_data, predictions)\n",
    "        elif data_loader[\"type\"] == \"mini-batch\": # calc loss and predictions over a batch of examples and do one update of m and c per batch.\n",
    "            if i > 0:\n",
    "                data_loader = data_loader[\"get_resetted_data_loader\"]()\n",
    "            \n",
    "            observed_examples = 0\n",
    "            running_loss = 0\n",
    "            for batch in data_loader[\"loader\"]():\n",
    "                indeces, examples = zip(*batch)\n",
    "                predictions, loss = calc_predictions_and_loss(m, c, indeces, examples)\n",
    "                running_loss += loss\n",
    "                m = m - lr * partial_derivative_mse_m(examples, predictions)\n",
    "                c = c - lr * partial_derivative_mse_c(examples, predictions)\n",
    "            \n",
    "            observed_examples += len(batch)\n",
    "            if observed_examples > 0:\n",
    "                loss_per_epoch.append(running_loss / observed_examples)\n",
    "        else: # calculate loss and prediction for one example and directly do an update of m and c. One update for each example.\n",
    "            if i > 0:\n",
    "                data_loader = data_loader[\"get_resetted_data_loader\"]()\n",
    "            \n",
    "            observed_examples = 0\n",
    "            running_loss = 0\n",
    "            for x, example in data_loader[\"loader\"]():\n",
    "                if amount != -1:\n",
    "                    if observed_examples >= amount:\n",
    "                        continue\n",
    "                \n",
    "                prediction, loss = calc_prediction_and_loss(m, c, x, example)\n",
    "                running_loss += loss\n",
    "                m = m - lr * partial_derivative_mse_m([example], [prediction])\n",
    "                c = c - lr * partial_derivative_mse_c([example], [prediction])\n",
    "                \n",
    "                observed_examples += 1\n",
    "                \n",
    "            if observed_examples > 0:\n",
    "                loss_per_epoch.append(running_loss / observed_examples)\n",
    "    return m, c, loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test shuffled data loader\n",
    "dl_shuffled = build_data_loader(training_data, params=dict(type=\"shuffled\"))\n",
    "for idx, val in dl_shuffled[\"loader\"]():\n",
    "    actual = training_data[idx]\n",
    "    if actual != val:\n",
    "        print(f\"  ERROR: value at index {idx} is {val} but should be {actual}\")\n",
    "\n",
    "# test mini-batch data loader\n",
    "dl_mini_batch = build_data_loader(training_data, params={\"type\": \"mini-batch\", \"batch-size\": 25})\n",
    "for batch in dl_mini_batch[\"loader\"]():\n",
    "    for idx, val in batch:\n",
    "        actual = training_data[idx]\n",
    "        if actual != val:\n",
    "            print(f\"  ERROR: value at index {idx} is {val} but should be {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5), gridspec_kw=dict(hspace=0.35))\n",
    "left = axs[0]\n",
    "right = axs[1]\n",
    "fig.suptitle(f\"Result of training the regression model for {epochs} epochs\", y=1.15)\n",
    "\n",
    "# You can play around with different epochs, learning rates, and data-loaders with different batch-sizes\n",
    "epochs = 300\n",
    "lr = 0.0015\n",
    "batch_size = 5\n",
    "\n",
    "# using no data loader performs one update per epoch\n",
    "# data_loader = None\n",
    "\n",
    "# linear data loader performs one parameter update per training example\n",
    "# data_loader = build_data_loader(training_data, params={\"type\": \"linear\"})\n",
    "\n",
    "# shuffled data loader also performs one parameter update per training example but the examples get shuffled randomly every epoch\n",
    "# data_loader = build_data_loader(training_data, params={\"type\": \"shuffled\"})\n",
    "\n",
    "# mini-batch data loader performs one update per batch of training examples and the examples get shuffled randomly every epoch\n",
    "data_loader = build_data_loader(training_data, params={\"type\": \"mini-batch\", \"batch-size\": batch_size})\n",
    "\n",
    "m, c, loss_per_epoch = train_regression_model(epochs=epochs, lr=lr, data_loader=data_loader)\n",
    "\n",
    "predictions, _ = calc_predictions_and_loss(m, c, X, training_data)\n",
    "left.scatter(X, training_data)\n",
    "left.plot(target_line, color=\"lime\", linewidth=4)\n",
    "left.plot(predictions, color=\"red\", linewidth=4)\n",
    "left.set_title(r\"training data around truth function $y=\\dfrac{3}{2}x-20$\" + \"\\n\"\n",
    "               + r\"and learned function $y=mx+c$\" + \"\\nwith m = \"\n",
    "               + f\"{round(m, 4)} c = {round(c, 4)}\")\n",
    "left.legend([\"truth line\", \"predicted line\", \"training data\"])\n",
    "left.set_xlabel(\"x\")\n",
    "left.set_ylabel(\"y\")\n",
    "\n",
    "right.plot(range(0, len(loss_per_epoch)), loss_per_epoch)\n",
    "right.set_title(f\"loss from Mean Squared Error\\nwith latest loss: {round(loss_per_epoch[-1], 4)}\")\n",
    "right.set_xlabel(\"epoch\")\n",
    "right.set_ylabel(\"loss\")\n",
    "\n",
    "# plt.savefig(\"./visualizations/gradient_descent_linear_regression_example.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
